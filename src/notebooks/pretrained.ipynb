{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/hayato/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/Users/hayato/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/Users/hayato/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/Users/hayato/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "dinov2_vits14 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DinoVisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x NestedTensorBlock(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): MemEffAttention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "  (head): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(dinov2_vits14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    DINOv2 ViT-S/14 model (optionally) pretrained on the LVD-142M dataset.\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/hayato/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "print(torch.hub.help(\"facebookresearch/dinov2\", \"dinov2_vits14\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 384, 16, 16]         226,176\n",
      "          Identity-2             [-1, 256, 384]               0\n",
      "        PatchEmbed-3             [-1, 256, 384]               0\n",
      "         LayerNorm-4             [-1, 257, 384]             768\n",
      "            Linear-5            [-1, 257, 1152]         443,520\n",
      "           Dropout-6          [-1, 6, 257, 257]               0\n",
      "            Linear-7             [-1, 257, 384]         147,840\n",
      "           Dropout-8             [-1, 257, 384]               0\n",
      "   MemEffAttention-9             [-1, 257, 384]               0\n",
      "       LayerScale-10             [-1, 257, 384]               0\n",
      "        LayerNorm-11             [-1, 257, 384]             768\n",
      "           Linear-12            [-1, 257, 1536]         591,360\n",
      "             GELU-13            [-1, 257, 1536]               0\n",
      "          Dropout-14            [-1, 257, 1536]               0\n",
      "           Linear-15             [-1, 257, 384]         590,208\n",
      "          Dropout-16             [-1, 257, 384]               0\n",
      "              Mlp-17             [-1, 257, 384]               0\n",
      "       LayerScale-18             [-1, 257, 384]               0\n",
      "NestedTensorBlock-19             [-1, 257, 384]               0\n",
      "        LayerNorm-20             [-1, 257, 384]             768\n",
      "           Linear-21            [-1, 257, 1152]         443,520\n",
      "          Dropout-22          [-1, 6, 257, 257]               0\n",
      "           Linear-23             [-1, 257, 384]         147,840\n",
      "          Dropout-24             [-1, 257, 384]               0\n",
      "  MemEffAttention-25             [-1, 257, 384]               0\n",
      "       LayerScale-26             [-1, 257, 384]               0\n",
      "        LayerNorm-27             [-1, 257, 384]             768\n",
      "           Linear-28            [-1, 257, 1536]         591,360\n",
      "             GELU-29            [-1, 257, 1536]               0\n",
      "          Dropout-30            [-1, 257, 1536]               0\n",
      "           Linear-31             [-1, 257, 384]         590,208\n",
      "          Dropout-32             [-1, 257, 384]               0\n",
      "              Mlp-33             [-1, 257, 384]               0\n",
      "       LayerScale-34             [-1, 257, 384]               0\n",
      "NestedTensorBlock-35             [-1, 257, 384]               0\n",
      "        LayerNorm-36             [-1, 257, 384]             768\n",
      "           Linear-37            [-1, 257, 1152]         443,520\n",
      "          Dropout-38          [-1, 6, 257, 257]               0\n",
      "           Linear-39             [-1, 257, 384]         147,840\n",
      "          Dropout-40             [-1, 257, 384]               0\n",
      "  MemEffAttention-41             [-1, 257, 384]               0\n",
      "       LayerScale-42             [-1, 257, 384]               0\n",
      "        LayerNorm-43             [-1, 257, 384]             768\n",
      "           Linear-44            [-1, 257, 1536]         591,360\n",
      "             GELU-45            [-1, 257, 1536]               0\n",
      "          Dropout-46            [-1, 257, 1536]               0\n",
      "           Linear-47             [-1, 257, 384]         590,208\n",
      "          Dropout-48             [-1, 257, 384]               0\n",
      "              Mlp-49             [-1, 257, 384]               0\n",
      "       LayerScale-50             [-1, 257, 384]               0\n",
      "NestedTensorBlock-51             [-1, 257, 384]               0\n",
      "        LayerNorm-52             [-1, 257, 384]             768\n",
      "           Linear-53            [-1, 257, 1152]         443,520\n",
      "          Dropout-54          [-1, 6, 257, 257]               0\n",
      "           Linear-55             [-1, 257, 384]         147,840\n",
      "          Dropout-56             [-1, 257, 384]               0\n",
      "  MemEffAttention-57             [-1, 257, 384]               0\n",
      "       LayerScale-58             [-1, 257, 384]               0\n",
      "        LayerNorm-59             [-1, 257, 384]             768\n",
      "           Linear-60            [-1, 257, 1536]         591,360\n",
      "             GELU-61            [-1, 257, 1536]               0\n",
      "          Dropout-62            [-1, 257, 1536]               0\n",
      "           Linear-63             [-1, 257, 384]         590,208\n",
      "          Dropout-64             [-1, 257, 384]               0\n",
      "              Mlp-65             [-1, 257, 384]               0\n",
      "       LayerScale-66             [-1, 257, 384]               0\n",
      "NestedTensorBlock-67             [-1, 257, 384]               0\n",
      "        LayerNorm-68             [-1, 257, 384]             768\n",
      "           Linear-69            [-1, 257, 1152]         443,520\n",
      "          Dropout-70          [-1, 6, 257, 257]               0\n",
      "           Linear-71             [-1, 257, 384]         147,840\n",
      "          Dropout-72             [-1, 257, 384]               0\n",
      "  MemEffAttention-73             [-1, 257, 384]               0\n",
      "       LayerScale-74             [-1, 257, 384]               0\n",
      "        LayerNorm-75             [-1, 257, 384]             768\n",
      "           Linear-76            [-1, 257, 1536]         591,360\n",
      "             GELU-77            [-1, 257, 1536]               0\n",
      "          Dropout-78            [-1, 257, 1536]               0\n",
      "           Linear-79             [-1, 257, 384]         590,208\n",
      "          Dropout-80             [-1, 257, 384]               0\n",
      "              Mlp-81             [-1, 257, 384]               0\n",
      "       LayerScale-82             [-1, 257, 384]               0\n",
      "NestedTensorBlock-83             [-1, 257, 384]               0\n",
      "        LayerNorm-84             [-1, 257, 384]             768\n",
      "           Linear-85            [-1, 257, 1152]         443,520\n",
      "          Dropout-86          [-1, 6, 257, 257]               0\n",
      "           Linear-87             [-1, 257, 384]         147,840\n",
      "          Dropout-88             [-1, 257, 384]               0\n",
      "  MemEffAttention-89             [-1, 257, 384]               0\n",
      "       LayerScale-90             [-1, 257, 384]               0\n",
      "        LayerNorm-91             [-1, 257, 384]             768\n",
      "           Linear-92            [-1, 257, 1536]         591,360\n",
      "             GELU-93            [-1, 257, 1536]               0\n",
      "          Dropout-94            [-1, 257, 1536]               0\n",
      "           Linear-95             [-1, 257, 384]         590,208\n",
      "          Dropout-96             [-1, 257, 384]               0\n",
      "              Mlp-97             [-1, 257, 384]               0\n",
      "       LayerScale-98             [-1, 257, 384]               0\n",
      "NestedTensorBlock-99             [-1, 257, 384]               0\n",
      "       LayerNorm-100             [-1, 257, 384]             768\n",
      "          Linear-101            [-1, 257, 1152]         443,520\n",
      "         Dropout-102          [-1, 6, 257, 257]               0\n",
      "          Linear-103             [-1, 257, 384]         147,840\n",
      "         Dropout-104             [-1, 257, 384]               0\n",
      " MemEffAttention-105             [-1, 257, 384]               0\n",
      "      LayerScale-106             [-1, 257, 384]               0\n",
      "       LayerNorm-107             [-1, 257, 384]             768\n",
      "          Linear-108            [-1, 257, 1536]         591,360\n",
      "            GELU-109            [-1, 257, 1536]               0\n",
      "         Dropout-110            [-1, 257, 1536]               0\n",
      "          Linear-111             [-1, 257, 384]         590,208\n",
      "         Dropout-112             [-1, 257, 384]               0\n",
      "             Mlp-113             [-1, 257, 384]               0\n",
      "      LayerScale-114             [-1, 257, 384]               0\n",
      "NestedTensorBlock-115             [-1, 257, 384]               0\n",
      "       LayerNorm-116             [-1, 257, 384]             768\n",
      "          Linear-117            [-1, 257, 1152]         443,520\n",
      "         Dropout-118          [-1, 6, 257, 257]               0\n",
      "          Linear-119             [-1, 257, 384]         147,840\n",
      "         Dropout-120             [-1, 257, 384]               0\n",
      " MemEffAttention-121             [-1, 257, 384]               0\n",
      "      LayerScale-122             [-1, 257, 384]               0\n",
      "       LayerNorm-123             [-1, 257, 384]             768\n",
      "          Linear-124            [-1, 257, 1536]         591,360\n",
      "            GELU-125            [-1, 257, 1536]               0\n",
      "         Dropout-126            [-1, 257, 1536]               0\n",
      "          Linear-127             [-1, 257, 384]         590,208\n",
      "         Dropout-128             [-1, 257, 384]               0\n",
      "             Mlp-129             [-1, 257, 384]               0\n",
      "      LayerScale-130             [-1, 257, 384]               0\n",
      "NestedTensorBlock-131             [-1, 257, 384]               0\n",
      "       LayerNorm-132             [-1, 257, 384]             768\n",
      "          Linear-133            [-1, 257, 1152]         443,520\n",
      "         Dropout-134          [-1, 6, 257, 257]               0\n",
      "          Linear-135             [-1, 257, 384]         147,840\n",
      "         Dropout-136             [-1, 257, 384]               0\n",
      " MemEffAttention-137             [-1, 257, 384]               0\n",
      "      LayerScale-138             [-1, 257, 384]               0\n",
      "       LayerNorm-139             [-1, 257, 384]             768\n",
      "          Linear-140            [-1, 257, 1536]         591,360\n",
      "            GELU-141            [-1, 257, 1536]               0\n",
      "         Dropout-142            [-1, 257, 1536]               0\n",
      "          Linear-143             [-1, 257, 384]         590,208\n",
      "         Dropout-144             [-1, 257, 384]               0\n",
      "             Mlp-145             [-1, 257, 384]               0\n",
      "      LayerScale-146             [-1, 257, 384]               0\n",
      "NestedTensorBlock-147             [-1, 257, 384]               0\n",
      "       LayerNorm-148             [-1, 257, 384]             768\n",
      "          Linear-149            [-1, 257, 1152]         443,520\n",
      "         Dropout-150          [-1, 6, 257, 257]               0\n",
      "          Linear-151             [-1, 257, 384]         147,840\n",
      "         Dropout-152             [-1, 257, 384]               0\n",
      " MemEffAttention-153             [-1, 257, 384]               0\n",
      "      LayerScale-154             [-1, 257, 384]               0\n",
      "       LayerNorm-155             [-1, 257, 384]             768\n",
      "          Linear-156            [-1, 257, 1536]         591,360\n",
      "            GELU-157            [-1, 257, 1536]               0\n",
      "         Dropout-158            [-1, 257, 1536]               0\n",
      "          Linear-159             [-1, 257, 384]         590,208\n",
      "         Dropout-160             [-1, 257, 384]               0\n",
      "             Mlp-161             [-1, 257, 384]               0\n",
      "      LayerScale-162             [-1, 257, 384]               0\n",
      "NestedTensorBlock-163             [-1, 257, 384]               0\n",
      "       LayerNorm-164             [-1, 257, 384]             768\n",
      "          Linear-165            [-1, 257, 1152]         443,520\n",
      "         Dropout-166          [-1, 6, 257, 257]               0\n",
      "          Linear-167             [-1, 257, 384]         147,840\n",
      "         Dropout-168             [-1, 257, 384]               0\n",
      " MemEffAttention-169             [-1, 257, 384]               0\n",
      "      LayerScale-170             [-1, 257, 384]               0\n",
      "       LayerNorm-171             [-1, 257, 384]             768\n",
      "          Linear-172            [-1, 257, 1536]         591,360\n",
      "            GELU-173            [-1, 257, 1536]               0\n",
      "         Dropout-174            [-1, 257, 1536]               0\n",
      "          Linear-175             [-1, 257, 384]         590,208\n",
      "         Dropout-176             [-1, 257, 384]               0\n",
      "             Mlp-177             [-1, 257, 384]               0\n",
      "      LayerScale-178             [-1, 257, 384]               0\n",
      "NestedTensorBlock-179             [-1, 257, 384]               0\n",
      "       LayerNorm-180             [-1, 257, 384]             768\n",
      "          Linear-181            [-1, 257, 1152]         443,520\n",
      "         Dropout-182          [-1, 6, 257, 257]               0\n",
      "          Linear-183             [-1, 257, 384]         147,840\n",
      "         Dropout-184             [-1, 257, 384]               0\n",
      " MemEffAttention-185             [-1, 257, 384]               0\n",
      "      LayerScale-186             [-1, 257, 384]               0\n",
      "       LayerNorm-187             [-1, 257, 384]             768\n",
      "          Linear-188            [-1, 257, 1536]         591,360\n",
      "            GELU-189            [-1, 257, 1536]               0\n",
      "         Dropout-190            [-1, 257, 1536]               0\n",
      "          Linear-191             [-1, 257, 384]         590,208\n",
      "         Dropout-192             [-1, 257, 384]               0\n",
      "             Mlp-193             [-1, 257, 384]               0\n",
      "      LayerScale-194             [-1, 257, 384]               0\n",
      "NestedTensorBlock-195             [-1, 257, 384]               0\n",
      "       LayerNorm-196             [-1, 257, 384]             768\n",
      "        Identity-197                  [-1, 384]               0\n",
      "================================================================\n",
      "Total params: 21,520,512\n",
      "Trainable params: 21,520,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 274.20\n",
      "Params size (MB): 82.09\n",
      "Estimated Total Size (MB): 356.87\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(dinov2_vits14, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "\n",
    "from lib.datasets import ThingsMEGDatasetWithImages\n",
    "\n",
    "import os\n",
    "\n",
    "import hydra\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from omegaconf import DictConfig\n",
    "from termcolor import cprint\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lib.datasets import ThingsMEGDataset\n",
    "from lib.models import BasicConvClassifier\n",
    "from lib.utils import set_seed\n",
    "\n",
    "data_dir = \"../../data\"\n",
    "batch_size = 32\n",
    "num_workers = 1\n",
    "loader_args = {\"batch_size\": batch_size, \"num_workers\": num_workers}\n",
    "\n",
    "train_set_with_images = ThingsMEGDatasetWithImages(\"train\", data_dir)\n",
    "train_loader_with_images = DataLoader(\n",
    "    train_set_with_images, shuffle=False, **loader_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224]) torch.Size([32, 271, 281]) torch.Size([32]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "torch.Size([32, 384])\n"
     ]
    }
   ],
   "source": [
    "for image_X, brain_X, y, subject_id in train_loader_with_images:\n",
    "    print(image_X.shape, brain_X.shape, y.shape, subject_id)\n",
    "    print(dinov2_vits14(image_X).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/hayato/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear4_head.pth\" to /Users/hayato/.cache/torch/hub/checkpoints/dinov2_vits14_linear4_head.pth\n",
      "100%|██████████| 7.33M/7.33M [00:00<00:00, 8.38MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_LinearClassifierWrapper(\n",
      "  (backbone): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (linear_head): Linear(in_features=1920, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dinov2_vits14_lc = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14_lc\")\n",
    "print(dinov2_vits14_lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 384, 16, 16]         226,176\n",
      "          Identity-2             [-1, 256, 384]               0\n",
      "        PatchEmbed-3             [-1, 256, 384]               0\n",
      "         LayerNorm-4             [-1, 257, 384]             768\n",
      "            Linear-5            [-1, 257, 1152]         443,520\n",
      "           Dropout-6          [-1, 6, 257, 257]               0\n",
      "            Linear-7             [-1, 257, 384]         147,840\n",
      "           Dropout-8             [-1, 257, 384]               0\n",
      "   MemEffAttention-9             [-1, 257, 384]               0\n",
      "       LayerScale-10             [-1, 257, 384]               0\n",
      "        LayerNorm-11             [-1, 257, 384]             768\n",
      "           Linear-12            [-1, 257, 1536]         591,360\n",
      "             GELU-13            [-1, 257, 1536]               0\n",
      "          Dropout-14            [-1, 257, 1536]               0\n",
      "           Linear-15             [-1, 257, 384]         590,208\n",
      "          Dropout-16             [-1, 257, 384]               0\n",
      "              Mlp-17             [-1, 257, 384]               0\n",
      "       LayerScale-18             [-1, 257, 384]               0\n",
      "NestedTensorBlock-19             [-1, 257, 384]               0\n",
      "        LayerNorm-20             [-1, 257, 384]             768\n",
      "           Linear-21            [-1, 257, 1152]         443,520\n",
      "          Dropout-22          [-1, 6, 257, 257]               0\n",
      "           Linear-23             [-1, 257, 384]         147,840\n",
      "          Dropout-24             [-1, 257, 384]               0\n",
      "  MemEffAttention-25             [-1, 257, 384]               0\n",
      "       LayerScale-26             [-1, 257, 384]               0\n",
      "        LayerNorm-27             [-1, 257, 384]             768\n",
      "           Linear-28            [-1, 257, 1536]         591,360\n",
      "             GELU-29            [-1, 257, 1536]               0\n",
      "          Dropout-30            [-1, 257, 1536]               0\n",
      "           Linear-31             [-1, 257, 384]         590,208\n",
      "          Dropout-32             [-1, 257, 384]               0\n",
      "              Mlp-33             [-1, 257, 384]               0\n",
      "       LayerScale-34             [-1, 257, 384]               0\n",
      "NestedTensorBlock-35             [-1, 257, 384]               0\n",
      "        LayerNorm-36             [-1, 257, 384]             768\n",
      "           Linear-37            [-1, 257, 1152]         443,520\n",
      "          Dropout-38          [-1, 6, 257, 257]               0\n",
      "           Linear-39             [-1, 257, 384]         147,840\n",
      "          Dropout-40             [-1, 257, 384]               0\n",
      "  MemEffAttention-41             [-1, 257, 384]               0\n",
      "       LayerScale-42             [-1, 257, 384]               0\n",
      "        LayerNorm-43             [-1, 257, 384]             768\n",
      "           Linear-44            [-1, 257, 1536]         591,360\n",
      "             GELU-45            [-1, 257, 1536]               0\n",
      "          Dropout-46            [-1, 257, 1536]               0\n",
      "           Linear-47             [-1, 257, 384]         590,208\n",
      "          Dropout-48             [-1, 257, 384]               0\n",
      "              Mlp-49             [-1, 257, 384]               0\n",
      "       LayerScale-50             [-1, 257, 384]               0\n",
      "NestedTensorBlock-51             [-1, 257, 384]               0\n",
      "        LayerNorm-52             [-1, 257, 384]             768\n",
      "           Linear-53            [-1, 257, 1152]         443,520\n",
      "          Dropout-54          [-1, 6, 257, 257]               0\n",
      "           Linear-55             [-1, 257, 384]         147,840\n",
      "          Dropout-56             [-1, 257, 384]               0\n",
      "  MemEffAttention-57             [-1, 257, 384]               0\n",
      "       LayerScale-58             [-1, 257, 384]               0\n",
      "        LayerNorm-59             [-1, 257, 384]             768\n",
      "           Linear-60            [-1, 257, 1536]         591,360\n",
      "             GELU-61            [-1, 257, 1536]               0\n",
      "          Dropout-62            [-1, 257, 1536]               0\n",
      "           Linear-63             [-1, 257, 384]         590,208\n",
      "          Dropout-64             [-1, 257, 384]               0\n",
      "              Mlp-65             [-1, 257, 384]               0\n",
      "       LayerScale-66             [-1, 257, 384]               0\n",
      "NestedTensorBlock-67             [-1, 257, 384]               0\n",
      "        LayerNorm-68             [-1, 257, 384]             768\n",
      "           Linear-69            [-1, 257, 1152]         443,520\n",
      "          Dropout-70          [-1, 6, 257, 257]               0\n",
      "           Linear-71             [-1, 257, 384]         147,840\n",
      "          Dropout-72             [-1, 257, 384]               0\n",
      "  MemEffAttention-73             [-1, 257, 384]               0\n",
      "       LayerScale-74             [-1, 257, 384]               0\n",
      "        LayerNorm-75             [-1, 257, 384]             768\n",
      "           Linear-76            [-1, 257, 1536]         591,360\n",
      "             GELU-77            [-1, 257, 1536]               0\n",
      "          Dropout-78            [-1, 257, 1536]               0\n",
      "           Linear-79             [-1, 257, 384]         590,208\n",
      "          Dropout-80             [-1, 257, 384]               0\n",
      "              Mlp-81             [-1, 257, 384]               0\n",
      "       LayerScale-82             [-1, 257, 384]               0\n",
      "NestedTensorBlock-83             [-1, 257, 384]               0\n",
      "        LayerNorm-84             [-1, 257, 384]             768\n",
      "           Linear-85            [-1, 257, 1152]         443,520\n",
      "          Dropout-86          [-1, 6, 257, 257]               0\n",
      "           Linear-87             [-1, 257, 384]         147,840\n",
      "          Dropout-88             [-1, 257, 384]               0\n",
      "  MemEffAttention-89             [-1, 257, 384]               0\n",
      "       LayerScale-90             [-1, 257, 384]               0\n",
      "        LayerNorm-91             [-1, 257, 384]             768\n",
      "           Linear-92            [-1, 257, 1536]         591,360\n",
      "             GELU-93            [-1, 257, 1536]               0\n",
      "          Dropout-94            [-1, 257, 1536]               0\n",
      "           Linear-95             [-1, 257, 384]         590,208\n",
      "          Dropout-96             [-1, 257, 384]               0\n",
      "              Mlp-97             [-1, 257, 384]               0\n",
      "       LayerScale-98             [-1, 257, 384]               0\n",
      "NestedTensorBlock-99             [-1, 257, 384]               0\n",
      "       LayerNorm-100             [-1, 257, 384]             768\n",
      "          Linear-101            [-1, 257, 1152]         443,520\n",
      "         Dropout-102          [-1, 6, 257, 257]               0\n",
      "          Linear-103             [-1, 257, 384]         147,840\n",
      "         Dropout-104             [-1, 257, 384]               0\n",
      " MemEffAttention-105             [-1, 257, 384]               0\n",
      "      LayerScale-106             [-1, 257, 384]               0\n",
      "       LayerNorm-107             [-1, 257, 384]             768\n",
      "          Linear-108            [-1, 257, 1536]         591,360\n",
      "            GELU-109            [-1, 257, 1536]               0\n",
      "         Dropout-110            [-1, 257, 1536]               0\n",
      "          Linear-111             [-1, 257, 384]         590,208\n",
      "         Dropout-112             [-1, 257, 384]               0\n",
      "             Mlp-113             [-1, 257, 384]               0\n",
      "      LayerScale-114             [-1, 257, 384]               0\n",
      "NestedTensorBlock-115             [-1, 257, 384]               0\n",
      "       LayerNorm-116             [-1, 257, 384]             768\n",
      "          Linear-117            [-1, 257, 1152]         443,520\n",
      "         Dropout-118          [-1, 6, 257, 257]               0\n",
      "          Linear-119             [-1, 257, 384]         147,840\n",
      "         Dropout-120             [-1, 257, 384]               0\n",
      " MemEffAttention-121             [-1, 257, 384]               0\n",
      "      LayerScale-122             [-1, 257, 384]               0\n",
      "       LayerNorm-123             [-1, 257, 384]             768\n",
      "          Linear-124            [-1, 257, 1536]         591,360\n",
      "            GELU-125            [-1, 257, 1536]               0\n",
      "         Dropout-126            [-1, 257, 1536]               0\n",
      "          Linear-127             [-1, 257, 384]         590,208\n",
      "         Dropout-128             [-1, 257, 384]               0\n",
      "             Mlp-129             [-1, 257, 384]               0\n",
      "      LayerScale-130             [-1, 257, 384]               0\n",
      "NestedTensorBlock-131             [-1, 257, 384]               0\n",
      "       LayerNorm-132             [-1, 257, 384]             768\n",
      "          Linear-133            [-1, 257, 1152]         443,520\n",
      "         Dropout-134          [-1, 6, 257, 257]               0\n",
      "          Linear-135             [-1, 257, 384]         147,840\n",
      "         Dropout-136             [-1, 257, 384]               0\n",
      " MemEffAttention-137             [-1, 257, 384]               0\n",
      "      LayerScale-138             [-1, 257, 384]               0\n",
      "       LayerNorm-139             [-1, 257, 384]             768\n",
      "          Linear-140            [-1, 257, 1536]         591,360\n",
      "            GELU-141            [-1, 257, 1536]               0\n",
      "         Dropout-142            [-1, 257, 1536]               0\n",
      "          Linear-143             [-1, 257, 384]         590,208\n",
      "         Dropout-144             [-1, 257, 384]               0\n",
      "             Mlp-145             [-1, 257, 384]               0\n",
      "      LayerScale-146             [-1, 257, 384]               0\n",
      "NestedTensorBlock-147             [-1, 257, 384]               0\n",
      "       LayerNorm-148             [-1, 257, 384]             768\n",
      "          Linear-149            [-1, 257, 1152]         443,520\n",
      "         Dropout-150          [-1, 6, 257, 257]               0\n",
      "          Linear-151             [-1, 257, 384]         147,840\n",
      "         Dropout-152             [-1, 257, 384]               0\n",
      " MemEffAttention-153             [-1, 257, 384]               0\n",
      "      LayerScale-154             [-1, 257, 384]               0\n",
      "       LayerNorm-155             [-1, 257, 384]             768\n",
      "          Linear-156            [-1, 257, 1536]         591,360\n",
      "            GELU-157            [-1, 257, 1536]               0\n",
      "         Dropout-158            [-1, 257, 1536]               0\n",
      "          Linear-159             [-1, 257, 384]         590,208\n",
      "         Dropout-160             [-1, 257, 384]               0\n",
      "             Mlp-161             [-1, 257, 384]               0\n",
      "      LayerScale-162             [-1, 257, 384]               0\n",
      "NestedTensorBlock-163             [-1, 257, 384]               0\n",
      "       LayerNorm-164             [-1, 257, 384]             768\n",
      "          Linear-165            [-1, 257, 1152]         443,520\n",
      "         Dropout-166          [-1, 6, 257, 257]               0\n",
      "          Linear-167             [-1, 257, 384]         147,840\n",
      "         Dropout-168             [-1, 257, 384]               0\n",
      " MemEffAttention-169             [-1, 257, 384]               0\n",
      "      LayerScale-170             [-1, 257, 384]               0\n",
      "       LayerNorm-171             [-1, 257, 384]             768\n",
      "          Linear-172            [-1, 257, 1536]         591,360\n",
      "            GELU-173            [-1, 257, 1536]               0\n",
      "         Dropout-174            [-1, 257, 1536]               0\n",
      "          Linear-175             [-1, 257, 384]         590,208\n",
      "         Dropout-176             [-1, 257, 384]               0\n",
      "             Mlp-177             [-1, 257, 384]               0\n",
      "      LayerScale-178             [-1, 257, 384]               0\n",
      "NestedTensorBlock-179             [-1, 257, 384]               0\n",
      "       LayerNorm-180             [-1, 257, 384]             768\n",
      "          Linear-181            [-1, 257, 1152]         443,520\n",
      "         Dropout-182          [-1, 6, 257, 257]               0\n",
      "          Linear-183             [-1, 257, 384]         147,840\n",
      "         Dropout-184             [-1, 257, 384]               0\n",
      " MemEffAttention-185             [-1, 257, 384]               0\n",
      "      LayerScale-186             [-1, 257, 384]               0\n",
      "       LayerNorm-187             [-1, 257, 384]             768\n",
      "          Linear-188            [-1, 257, 1536]         591,360\n",
      "            GELU-189            [-1, 257, 1536]               0\n",
      "         Dropout-190            [-1, 257, 1536]               0\n",
      "          Linear-191             [-1, 257, 384]         590,208\n",
      "         Dropout-192             [-1, 257, 384]               0\n",
      "             Mlp-193             [-1, 257, 384]               0\n",
      "      LayerScale-194             [-1, 257, 384]               0\n",
      "NestedTensorBlock-195             [-1, 257, 384]               0\n",
      "       LayerNorm-196             [-1, 257, 384]             768\n",
      "       LayerNorm-197             [-1, 257, 384]             768\n",
      "       LayerNorm-198             [-1, 257, 384]             768\n",
      "       LayerNorm-199             [-1, 257, 384]             768\n",
      "          Linear-200                 [-1, 1000]       1,921,000\n",
      "================================================================\n",
      "Total params: 23,443,816\n",
      "Trainable params: 23,443,816\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 276.47\n",
      "Params size (MB): 89.43\n",
      "Estimated Total Size (MB): 366.47\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(dinov2_vits14_lc, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /Users/hayato/.cache/torch/hub/v0.10.0.zip\n",
      "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /Users/hayato/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
      "100%|██████████| 230M/230M [00:04<00:00, 50.3MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-80          [-1, 128, 28, 28]             256\n",
      "             ReLU-81          [-1, 128, 28, 28]               0\n",
      "           Conv2d-82          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-83          [-1, 128, 28, 28]             256\n",
      "             ReLU-84          [-1, 128, 28, 28]               0\n",
      "           Conv2d-85          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-86          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-87          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-88          [-1, 512, 28, 28]               0\n",
      "           Conv2d-89          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-90          [-1, 128, 28, 28]             256\n",
      "             ReLU-91          [-1, 128, 28, 28]               0\n",
      "           Conv2d-92          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-93          [-1, 128, 28, 28]             256\n",
      "             ReLU-94          [-1, 128, 28, 28]               0\n",
      "           Conv2d-95          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-96          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-97          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-98          [-1, 512, 28, 28]               0\n",
      "           Conv2d-99          [-1, 128, 28, 28]          65,536\n",
      "     BatchNorm2d-100          [-1, 128, 28, 28]             256\n",
      "            ReLU-101          [-1, 128, 28, 28]               0\n",
      "          Conv2d-102          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-103          [-1, 128, 28, 28]             256\n",
      "            ReLU-104          [-1, 128, 28, 28]               0\n",
      "          Conv2d-105          [-1, 512, 28, 28]          65,536\n",
      "     BatchNorm2d-106          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-107          [-1, 512, 28, 28]               0\n",
      "      Bottleneck-108          [-1, 512, 28, 28]               0\n",
      "          Conv2d-109          [-1, 128, 28, 28]          65,536\n",
      "     BatchNorm2d-110          [-1, 128, 28, 28]             256\n",
      "            ReLU-111          [-1, 128, 28, 28]               0\n",
      "          Conv2d-112          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-113          [-1, 128, 28, 28]             256\n",
      "            ReLU-114          [-1, 128, 28, 28]               0\n",
      "          Conv2d-115          [-1, 512, 28, 28]          65,536\n",
      "     BatchNorm2d-116          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-117          [-1, 512, 28, 28]               0\n",
      "      Bottleneck-118          [-1, 512, 28, 28]               0\n",
      "          Conv2d-119          [-1, 256, 28, 28]         131,072\n",
      "     BatchNorm2d-120          [-1, 256, 28, 28]             512\n",
      "            ReLU-121          [-1, 256, 28, 28]               0\n",
      "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
      "            ReLU-124          [-1, 256, 14, 14]               0\n",
      "          Conv2d-125         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-126         [-1, 1024, 14, 14]           2,048\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         524,288\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
      "            ReLU-143          [-1, 256, 14, 14]               0\n",
      "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
      "            ReLU-146          [-1, 256, 14, 14]               0\n",
      "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-149         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-150         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-151          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-152          [-1, 256, 14, 14]             512\n",
      "            ReLU-153          [-1, 256, 14, 14]               0\n",
      "          Conv2d-154          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-155          [-1, 256, 14, 14]             512\n",
      "            ReLU-156          [-1, 256, 14, 14]               0\n",
      "          Conv2d-157         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-159         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-160         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-161          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-162          [-1, 256, 14, 14]             512\n",
      "            ReLU-163          [-1, 256, 14, 14]               0\n",
      "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
      "            ReLU-166          [-1, 256, 14, 14]               0\n",
      "          Conv2d-167         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-168         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-169         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-170         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-171          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-172          [-1, 256, 14, 14]             512\n",
      "            ReLU-173          [-1, 256, 14, 14]               0\n",
      "          Conv2d-174          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
      "            ReLU-176          [-1, 256, 14, 14]               0\n",
      "          Conv2d-177         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-178         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-179         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-180         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-181          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
      "            ReLU-183          [-1, 256, 14, 14]               0\n",
      "          Conv2d-184          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-185          [-1, 256, 14, 14]             512\n",
      "            ReLU-186          [-1, 256, 14, 14]               0\n",
      "          Conv2d-187         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-188         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-189         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-190         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-191          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-192          [-1, 256, 14, 14]             512\n",
      "            ReLU-193          [-1, 256, 14, 14]               0\n",
      "          Conv2d-194          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-195          [-1, 256, 14, 14]             512\n",
      "            ReLU-196          [-1, 256, 14, 14]               0\n",
      "          Conv2d-197         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-199         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-200         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-201          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-202          [-1, 256, 14, 14]             512\n",
      "            ReLU-203          [-1, 256, 14, 14]               0\n",
      "          Conv2d-204          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-205          [-1, 256, 14, 14]             512\n",
      "            ReLU-206          [-1, 256, 14, 14]               0\n",
      "          Conv2d-207         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-209         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-210         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-211          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-212          [-1, 256, 14, 14]             512\n",
      "            ReLU-213          [-1, 256, 14, 14]               0\n",
      "          Conv2d-214          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-215          [-1, 256, 14, 14]             512\n",
      "            ReLU-216          [-1, 256, 14, 14]               0\n",
      "          Conv2d-217         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-219         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-220         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-221          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-222          [-1, 256, 14, 14]             512\n",
      "            ReLU-223          [-1, 256, 14, 14]               0\n",
      "          Conv2d-224          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-225          [-1, 256, 14, 14]             512\n",
      "            ReLU-226          [-1, 256, 14, 14]               0\n",
      "          Conv2d-227         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-229         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-230         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-231          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-232          [-1, 256, 14, 14]             512\n",
      "            ReLU-233          [-1, 256, 14, 14]               0\n",
      "          Conv2d-234          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-235          [-1, 256, 14, 14]             512\n",
      "            ReLU-236          [-1, 256, 14, 14]               0\n",
      "          Conv2d-237         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-239         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-240         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-241          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
      "            ReLU-243          [-1, 256, 14, 14]               0\n",
      "          Conv2d-244          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-245          [-1, 256, 14, 14]             512\n",
      "            ReLU-246          [-1, 256, 14, 14]               0\n",
      "          Conv2d-247         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-249         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-250         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-251          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
      "            ReLU-253          [-1, 256, 14, 14]               0\n",
      "          Conv2d-254          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-255          [-1, 256, 14, 14]             512\n",
      "            ReLU-256          [-1, 256, 14, 14]               0\n",
      "          Conv2d-257         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-259         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-260         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-261          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-262          [-1, 256, 14, 14]             512\n",
      "            ReLU-263          [-1, 256, 14, 14]               0\n",
      "          Conv2d-264          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-265          [-1, 256, 14, 14]             512\n",
      "            ReLU-266          [-1, 256, 14, 14]               0\n",
      "          Conv2d-267         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-269         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-270         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-271          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-272          [-1, 256, 14, 14]             512\n",
      "            ReLU-273          [-1, 256, 14, 14]               0\n",
      "          Conv2d-274          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-275          [-1, 256, 14, 14]             512\n",
      "            ReLU-276          [-1, 256, 14, 14]               0\n",
      "          Conv2d-277         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-279         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-280         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-281          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-282          [-1, 256, 14, 14]             512\n",
      "            ReLU-283          [-1, 256, 14, 14]               0\n",
      "          Conv2d-284          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-285          [-1, 256, 14, 14]             512\n",
      "            ReLU-286          [-1, 256, 14, 14]               0\n",
      "          Conv2d-287         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-289         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-290         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-291          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
      "            ReLU-293          [-1, 256, 14, 14]               0\n",
      "          Conv2d-294          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-295          [-1, 256, 14, 14]             512\n",
      "            ReLU-296          [-1, 256, 14, 14]               0\n",
      "          Conv2d-297         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-299         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-300         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-301          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-302          [-1, 256, 14, 14]             512\n",
      "            ReLU-303          [-1, 256, 14, 14]               0\n",
      "          Conv2d-304          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-305          [-1, 256, 14, 14]             512\n",
      "            ReLU-306          [-1, 256, 14, 14]               0\n",
      "          Conv2d-307         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-309         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-310         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-311          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-312          [-1, 256, 14, 14]             512\n",
      "            ReLU-313          [-1, 256, 14, 14]               0\n",
      "          Conv2d-314          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-315          [-1, 256, 14, 14]             512\n",
      "            ReLU-316          [-1, 256, 14, 14]               0\n",
      "          Conv2d-317         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-318         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-319         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-320         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-321          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-322          [-1, 256, 14, 14]             512\n",
      "            ReLU-323          [-1, 256, 14, 14]               0\n",
      "          Conv2d-324          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-325          [-1, 256, 14, 14]             512\n",
      "            ReLU-326          [-1, 256, 14, 14]               0\n",
      "          Conv2d-327         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-328         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-329         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-330         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-331          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-332          [-1, 256, 14, 14]             512\n",
      "            ReLU-333          [-1, 256, 14, 14]               0\n",
      "          Conv2d-334          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-335          [-1, 256, 14, 14]             512\n",
      "            ReLU-336          [-1, 256, 14, 14]               0\n",
      "          Conv2d-337         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-338         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-339         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-340         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-341          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-342          [-1, 256, 14, 14]             512\n",
      "            ReLU-343          [-1, 256, 14, 14]               0\n",
      "          Conv2d-344          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-345          [-1, 256, 14, 14]             512\n",
      "            ReLU-346          [-1, 256, 14, 14]               0\n",
      "          Conv2d-347         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-348         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-349         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-350         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-351          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-352          [-1, 256, 14, 14]             512\n",
      "            ReLU-353          [-1, 256, 14, 14]               0\n",
      "          Conv2d-354          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-355          [-1, 256, 14, 14]             512\n",
      "            ReLU-356          [-1, 256, 14, 14]               0\n",
      "          Conv2d-357         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-358         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-359         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-360         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-361          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-362          [-1, 256, 14, 14]             512\n",
      "            ReLU-363          [-1, 256, 14, 14]               0\n",
      "          Conv2d-364          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-365          [-1, 256, 14, 14]             512\n",
      "            ReLU-366          [-1, 256, 14, 14]               0\n",
      "          Conv2d-367         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-368         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-369         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-370         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-371          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-372          [-1, 256, 14, 14]             512\n",
      "            ReLU-373          [-1, 256, 14, 14]               0\n",
      "          Conv2d-374          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-375          [-1, 256, 14, 14]             512\n",
      "            ReLU-376          [-1, 256, 14, 14]               0\n",
      "          Conv2d-377         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-378         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-379         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-380         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-381          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-382          [-1, 256, 14, 14]             512\n",
      "            ReLU-383          [-1, 256, 14, 14]               0\n",
      "          Conv2d-384          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-385          [-1, 256, 14, 14]             512\n",
      "            ReLU-386          [-1, 256, 14, 14]               0\n",
      "          Conv2d-387         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-388         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-389         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-390         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-391          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-392          [-1, 256, 14, 14]             512\n",
      "            ReLU-393          [-1, 256, 14, 14]               0\n",
      "          Conv2d-394          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-395          [-1, 256, 14, 14]             512\n",
      "            ReLU-396          [-1, 256, 14, 14]               0\n",
      "          Conv2d-397         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-398         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-399         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-400         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-401          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-402          [-1, 256, 14, 14]             512\n",
      "            ReLU-403          [-1, 256, 14, 14]               0\n",
      "          Conv2d-404          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-405          [-1, 256, 14, 14]             512\n",
      "            ReLU-406          [-1, 256, 14, 14]               0\n",
      "          Conv2d-407         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-408         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-409         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-410         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-411          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-412          [-1, 256, 14, 14]             512\n",
      "            ReLU-413          [-1, 256, 14, 14]               0\n",
      "          Conv2d-414          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-415          [-1, 256, 14, 14]             512\n",
      "            ReLU-416          [-1, 256, 14, 14]               0\n",
      "          Conv2d-417         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-418         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-419         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-420         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-421          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-422          [-1, 256, 14, 14]             512\n",
      "            ReLU-423          [-1, 256, 14, 14]               0\n",
      "          Conv2d-424          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-425          [-1, 256, 14, 14]             512\n",
      "            ReLU-426          [-1, 256, 14, 14]               0\n",
      "          Conv2d-427         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-428         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-429         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-430         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-431          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-432          [-1, 256, 14, 14]             512\n",
      "            ReLU-433          [-1, 256, 14, 14]               0\n",
      "          Conv2d-434          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-435          [-1, 256, 14, 14]             512\n",
      "            ReLU-436          [-1, 256, 14, 14]               0\n",
      "          Conv2d-437         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-438         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-439         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-440         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-441          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-442          [-1, 256, 14, 14]             512\n",
      "            ReLU-443          [-1, 256, 14, 14]               0\n",
      "          Conv2d-444          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-445          [-1, 256, 14, 14]             512\n",
      "            ReLU-446          [-1, 256, 14, 14]               0\n",
      "          Conv2d-447         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-448         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-449         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-450         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-451          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-452          [-1, 256, 14, 14]             512\n",
      "            ReLU-453          [-1, 256, 14, 14]               0\n",
      "          Conv2d-454          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-455          [-1, 256, 14, 14]             512\n",
      "            ReLU-456          [-1, 256, 14, 14]               0\n",
      "          Conv2d-457         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-458         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-459         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-460         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-461          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-462          [-1, 256, 14, 14]             512\n",
      "            ReLU-463          [-1, 256, 14, 14]               0\n",
      "          Conv2d-464          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-465          [-1, 256, 14, 14]             512\n",
      "            ReLU-466          [-1, 256, 14, 14]               0\n",
      "          Conv2d-467         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-468         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-469         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-470         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-471          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-472          [-1, 256, 14, 14]             512\n",
      "            ReLU-473          [-1, 256, 14, 14]               0\n",
      "          Conv2d-474          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-475          [-1, 256, 14, 14]             512\n",
      "            ReLU-476          [-1, 256, 14, 14]               0\n",
      "          Conv2d-477         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-478         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-479         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-480         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-481          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-482          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-483          [-1, 512, 14, 14]               0\n",
      "          Conv2d-484            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-485            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-486            [-1, 512, 7, 7]               0\n",
      "          Conv2d-487           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-488           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-489           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-490           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-491           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-492           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-493            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-494            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-495            [-1, 512, 7, 7]               0\n",
      "          Conv2d-496            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-497            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-498            [-1, 512, 7, 7]               0\n",
      "          Conv2d-499           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-500           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-501           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-502           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-503            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-504            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-505            [-1, 512, 7, 7]               0\n",
      "          Conv2d-506            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-507            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-508            [-1, 512, 7, 7]               0\n",
      "          Conv2d-509           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-510           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-511           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-512           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-513           [-1, 2048, 1, 1]               0\n",
      "          Linear-514                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 60,192,808\n",
      "Trainable params: 60,192,808\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 606.59\n",
      "Params size (MB): 229.62\n",
      "Estimated Total Size (MB): 836.78\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load(\"pytorch/vision:v0.10.0\", \"resnet152\", pretrained=True)\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
